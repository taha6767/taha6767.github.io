I am working with Proffessor Aashita Kesarwani to make a chatbot that is able to answer IT related questions for Harvey Mudd students using Retriaval Augmented Generation.

How it works: First I webscraped the IT Helpdesk website of Harvey Mudd and recorded all the related articles.

Web scraping was made possible thanks to the knowledge I gained in the Fundamentals of Data Science class. We used the scraped data as input for a large language model (LLM), which generated a high-dimensional vector representing its understanding of the data. When a user asked a question, we embedded their prompt using the same LLM and compared it to the vectors derived from our knowledgebase articles. By identifying the most similar knowledgebase article to the userâ€™s input, we used it as a reference to provide precise and accurate answers.

Currently, we are experimenting with various LLMs, such as ChatGPT-4, Mistral, and Claude 3.5. I will continue to update this website as we make progress on the project.

Here is the code I am using for webscraping:

```{# # First, run this cell to install required libraries if you haven't already}
# # !pip install requests beautifulsoup4 ftfy
# 
# import requests
# from bs4 import BeautifulSoup
# import time
# import csv
# from urllib.parse import urljoin
# from datetime import datetime
# from google.colab import files
# import io
# import ftfy
# import unicodedata
# 
# # Main URL
# base_url = "https://www.hmc.edu/cis/services/"
# 
# def clean_text(text):
#     """Clean and normalize text to handle encoding issues"""
#     if not text:
#         return ""
#     text = ftfy.fix_text(text)
#     text = unicodedata.normalize('NFKD', text)
#     text = text.replace('\u2019', "'")
#     text = text.replace('\u201c', '"')
#     text = text.replace('\u201d', '"')
#     text = text.replace('\u2013', '-')
#     text = text.replace('\u2014', '-')
#     return text
# 
# def get_service_links(url):
#     """Get all service links in A-Z order as they appear on the page"""
#     response = requests.get(url)
#     response.encoding = 'utf-8'
#     soup = BeautifulSoup(response.text, "html.parser")
# 
#     links_with_text = []
#     print("Debugging - Found links:")
# 
#     content_wrapper = soup.find('div', {'id': 'content-wrapper'})
#     if content_wrapper:
#         for link in content_wrapper.find_all('a', href=True):
#             href = link.get('href')
#             text = clean_text(link.get_text().strip())
#             if href and text:
#                 full_url = urljoin(base_url, href)
#                 links_with_text.append((text, full_url))
#                 print(f"Found link: {text} -> {full_url}")
# 
#     ordered_links = [url for text, url in links_with_text]
#     print(f"\nTotal links found (in order): {len(ordered_links)}")
#     return ordered_links
# 
# def process_text_with_links(soup, title_text=""):
#     """Process text while preserving links inline"""
#     # Remove right sidebar elements
#     sidebar = soup.find('div', {'id': 'right-sidebar'})
#     if sidebar:
#         # Remove list items, strong elements, and headings from sidebar
#         for element in sidebar.select('li, strong, .wp-block-heading'):
#             element.decompose()
# 
#     # Remove script and style tags
#     for tag in soup.find_all(['script', 'style']):
#         tag.decompose()
# 
#     for a in soup.find_all('a', href=True):
#         href = a.get('href')
#         if href:
#             if not href.startswith(('http', '/')):
#                 href = urljoin(base_url, href)
#             a.string = f"{a.get_text()} <{href}>"
# 
#     # Combine title and content
#     main_content = soup.get_text()
#     combined_content = f"{title_text}\n\n{main_content}" if title_text else main_content
#     return clean_text(combined_content)
# 
# def extract_page_content(soup):
#     """Extract content from specific selectors"""
#     content = {
#         'page_url': '',  # Will be set later
#         'title': '',
#         'content': ''
#     }
# 
#     # Get the title using .mb-0 class
#     title = soup.find(class_='mb-0')
#     if title:
#         title_text = clean_text(title.get_text().strip())
#         content['title'] = title_text
# 
#     # Get main content
#     content_wrapper = soup.find('div', {'id': 'content-wrapper'})
#     if content_wrapper:
#         # Process the content while preserving links and including title
#         content['content'] = process_text_with_links(content_wrapper, content['title'])
# 
#     return content
# 
# def scrape_links_to_colab():
#     """Scrape content and save to CSV, then download in Colab"""
#     output = io.StringIO()
#     fieldnames = ['page_url', 'title', 'content']
#     writer = csv.DictWriter(output, fieldnames=fieldnames)
#     writer.writeheader()
# 
#     print("Getting service links...")
#     service_links = get_service_links(base_url)
# 
#     if not service_links:
#         print("No service links found. Adding base URL to scrape...")
#         service_links = [base_url]
# 
#     # Only take the first 10 links
#     print(f"Limited to first 10 links for debugging")
# 
#     for link in service_links:
#         try:
#             print(f"\nVisiting: {link}")
#             response = requests.get(link)
#             response.encoding = 'utf-8'
#             soup = BeautifulSoup(response.text, "html.parser")
# 
#             content = extract_page_content(soup)
#             content['page_url'] = link
# 
#             writer.writerow(content)
#             print(f"Successfully scraped page: {link}")
#             time.sleep(1)
# 
#         except Exception as e:
#             print(f"Error visiting {link}: {e}")
#             writer.writerow({
#                 'page_url': link,
#                 'title': 'ERROR',
#                 'content': f"ERROR: {str(e)}"
#             })
# 
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"scraped_content_{timestamp}.csv"
# 
#     with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
#         f.write(output.getvalue())
# 
#     files.download(filename)
#     print(f"\nScraping completed. File '{filename}' has been downloaded.")
# 
# # Run the scraper
# print("Starting the scraper...")
# scrape_links_to_colab() 
```

This Python script is a web scraper designed to extract content from Harvey Mudd College's CIS services website. It crawls through service-related pages, collecting the title, content, and URLs while preserving any links found within the text. The script cleans and normalizes the text to handle encoding issues, and removes unwanted elements like scripts and sidebars. Finally, it saves all the scraped data into a CSV file with a timestamp in the filename, which can be downloaded directly from Google Colab.
